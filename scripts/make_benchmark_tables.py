#!/usr/bin/env python3
# scripts/make_benchmark_tables.py
from __future__ import annotations
from pathlib import Path
import argparse, json, itertools, sys, re, warnings
import numpy as np
import pandas as pd
from scipy import stats

# ---------------------------- utilities ---------------------------------

CORE = ["eccentricity","mean_motion","semi_major_axis"]

def clip_pos(a, eps=1e-12):
    a = np.asarray(a, float).copy()
    a[a<=0] = eps
    return a

def gmean_pos(x):
    x = np.asarray(x, float)
    x = clip_pos(x)
    return float(np.exp(np.log(x).mean())) if x.size else np.nan

def q(x, p):
    return float(np.nanquantile(np.asarray(x, float), p)) if len(x) else np.nan

def make_out(root:Path, name:str)->Path:
    p = root/name; p.mkdir(parents=True, exist_ok=True); return p

def find_metrics_file(root:Path)->Path|None:
    for cand in ["bench_metrics.csv","metrics_linear.csv","matrix_linear.csv","metrics.csv"]:
        p = root/cand
        if p.exists(): return p
    return None

def load_optional_csv(p:Path)->pd.DataFrame:
    try:
        if p.exists(): return pd.read_csv(p)
    except Exception as e:
        warnings.warn(f"Failed to read {p}: {e}")
    return pd.DataFrame()

def save_table(df:pd.DataFrame, outdir:Path, name:str, index:bool=True,
               floatfmt:str="{:.6g}", latex_bold_min_cols:list[str]|None=None,
               caption:str|None=None, label:str|None=None):
    """Write CSV and LaTeX versions. Optionally bold the minimum in specified columns (per column)."""
    outdir.mkdir(parents=True, exist_ok=True)

    # CSV
    csv_path = outdir/f"{name}.csv"
    df.to_csv(csv_path, index=index)

    # LaTeX
    df_lx = df.copy()
    if latex_bold_min_cols:
        for c in latex_bold_min_cols:
            if c in df_lx.columns:
                try:
                    col = pd.to_numeric(df_lx[c], errors="coerce")
                    if col.notna().any():
                        m = col.min()
                        df_lx[c] = [
                            f"\\textbf{{{floatfmt.format(v)}}}" if pd.notna(v) and abs(v - m) < 1e-15
                            else (floatfmt.format(v) if pd.notna(v) else "")
                            for v in col
                        ]
                except Exception:
                    pass

    # format floats elementwise
    def fmt_val(v):
        if isinstance(v, (int, np.integer)):
            return str(v)
        try:
            fv = float(v)
            return floatfmt.format(fv)
        except Exception:
            return str(v)

    # pandas >= 2.2 recommends DataFrame.map; fall back to applymap if needed
    if hasattr(pd.DataFrame, "map"):
        df_lx = df_lx.map(fmt_val)
    else:
        df_lx = df_lx.applymap(fmt_val)

    latex_path = outdir/f"{name}.tex"
    with open(latex_path, "w") as f:
        cap = caption or name.replace("_"," ").title()
        lab = label or f"tab:{name}"
        f.write("% Auto-generated by make_benchmark_tables.py\n")
        f.write("\\begin{table}[t]\n\\centering\n")

        colspec = ("l" + "r"*df_lx.shape[1]) if index else ("r"*df_lx.shape[1])
        f.write("\\begin{tabular}{%s}\n" % colspec)  # <-- ensure ternary applies inside % formatting
        f.write("\\hline\n")

        # --- header + rows (make sure everything is str) ---
        if index:
            header = [str(df_lx.index.name or "")] + [str(c) for c in df_lx.columns]
            f.write(" & ".join(header) + " \\\\\n\\hline\n")
            for idx, row in df_lx.iterrows():
                row_str = [str(idx)] + [str(v) for v in row.values]
                f.write(" & ".join(row_str) + " \\\\\n")
        else:
            header = [str(c) for c in df_lx.columns]
            f.write(" & ".join(header) + " \\\\\n\\hline\n")
            for _, row in df_lx.iterrows():
                row_str = [str(v) for v in row.values]
                f.write(" & ".join(row_str) + " \\\\\n")


        f.write("\\hline\n\\end{tabular}\n")
        # avoid f-string here to prevent `{table}` interpolation errors
        f.write("\\caption{%s}\n\\label{%s}\n\\end{table}\n" % (cap, lab))

    print(f"üìÑ  wrote {csv_path}")
    print(f"üìÑ  wrote {latex_path}")


def safe_cols(df:pd.DataFrame, cols:list[str])->bool:
    return set(cols).issubset(df.columns)

def bootstrap_ci_gmean(values, B=1000, alpha=0.05, seed=42):
    values = np.asarray(values, float)
    values = values[np.isfinite(values)]
    if values.size == 0: return (np.nan, np.nan, np.nan)
    rng = np.random.default_rng(seed)
    boots = [gmean_pos(rng.choice(values, size=len(values), replace=True)) for _ in range(B)]
    lo, hi = np.quantile(boots, [alpha/2., 1-alpha/2.])
    return (gmean_pos(values), lo, hi)

# ----------------------------- main -------------------------------------

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--root", type=Path, default=Path("results/benchmark_v2"))
    ap.add_argument("--baseline", default="sgp4_monotonic")
    ap.add_argument("--span-order", nargs="*", default=["last90d","last180d","last365d","last730d","full"])
    ap.add_argument("--out", default="tables")
    args = ap.parse_args()

    root:Path = args.root
    TOUT = make_out(root, args.out)

    # Load main linear metrics
    met_path = find_metrics_file(root)
    if not met_path:
        print("‚ùå No metrics file found (looked for bench_metrics.csv, metrics_linear.csv, matrix_linear.csv).", file=sys.stderr)
        sys.exit(2)
    dmet = load_optional_csv(met_path)
    if dmet.empty:
        print(f"‚ùå {met_path} is empty or unreadable.", file=sys.stderr)
        sys.exit(2)

    # Try additional sources for richer tables
    dlog  = load_optional_csv(root/"bench_metrics_log.csv")
    dsamp = load_optional_csv(root/"bench_samples.csv")
    prof  = load_optional_csv(Path("results/profiles")/"profiles.csv")  # optional convenience if someone wrote a csv
    # Also ingest any JSON profiles (params/latency) as in your plotting script
    if prof.empty:
        prof_dir = Path("results/profiles")
        rows=[]
        if prof_dir.exists():
            for f in prof_dir.glob("*.json"):
                try:
                    d = json.loads(f.read_text())
                    d["model"] = f.stem
                    rows.append(d)
                except: pass
        if rows:
            prof = pd.DataFrame(rows)

    # Basic columns sanity
    if "model" not in dmet.columns or "feature" not in dmet.columns:
        print("‚ùå metrics file must include at least 'model' and 'feature' columns.", file=sys.stderr)
        sys.exit(2)

    # Normalize column names a bit
    for c in ["h","H","horizon_days","horizon"]:
        if c in dmet.columns and "h" not in dmet.columns:
            dmet = dmet.rename(columns={c:"h"})
            break
    for c in ["span","lookback","look_back"]:
        if c in dmet.columns and "span" not in dmet.columns:
            dmet = dmet.rename(columns={c:"span"})
            break
    for c in ["window","w","win"]:
        if c in dmet.columns and "w" not in dmet.columns:
            dmet = dmet.rename(columns={c:"w"})
            break
    # ensure RMSE/MAPE names
    if "RMSE" not in dmet.columns:
        # sometimes named 'rmse'
        if "rmse" in dmet.columns: dmet = dmet.rename(columns={"rmse":"RMSE"})
    if "MAPE" not in dmet.columns and "mape" in dmet.columns:
        dmet = dmet.rename(columns={"mape":"MAPE"})

    models = sorted(dmet["model"].unique())
    if args.baseline not in set(models):
        # auto-detect SGP4 baseline
        cand = next((m for m in models if re.search("sgp4", m, re.I)), None)
        if cand:
            args.baseline = cand
            print(f"‚ÑπÔ∏è  baseline auto-set to {args.baseline}")
    baseline = args.baseline

    # ----------------------------- TABLES --------------------------------
    # Table set A: overall rankings
    def agg_rmse(df): return df.groupby("model").RMSE.apply(gmean_pos).sort_values()

    # A1: Overall ranking by RMSE (core features only)
    if "RMSE" in dmet.columns:
        dcore = dmet[dmet.feature.isin(CORE)] if set(CORE).issubset(dmet.feature.unique()) else dmet
        best = agg_rmse(dcore).to_frame(name="RMSE_geo")
        best["Œî_vs_baseline_%"] = 100.0 * (best["RMSE_geo"] / best.loc[baseline, "RMSE_geo"] - 1.0) if baseline in best.index else np.nan
        save_table(best, TOUT, "A1_overall_ranking_rmse", index=True, latex_bold_min_cols=["RMSE_geo"],
                   caption="Overall ranking by geometric-mean RMSE (core features).")

    # A2: Overall ranking by MAPE (if available)
    if "MAPE" in dmet.columns:
        best_m = dcore.groupby("model").MAPE.apply(gmean_pos).sort_values().to_frame(name="MAPE_geo")
        save_table(best_m, TOUT, "A2_overall_ranking_mape", index=True, latex_bold_min_cols=["MAPE_geo"],
                   caption="Overall ranking by geometric-mean MAPE (core features).")

    # Table set B: per-feature breakdown
    # B1: RMSE by feature (geo-mean across everything else)
    if "RMSE" in dmet.columns:
        feat_tbl = (dmet.groupby(["model","feature"]).RMSE.apply(gmean_pos).reset_index()
                        .pivot(index="model", columns="feature", values="RMSE"))
        save_table(feat_tbl, TOUT, "B1_rmse_by_feature", index=True, latex_bold_min_cols=list(feat_tbl.columns),
                   caption="Geometric-mean RMSE by feature.")

    # B2: MAPE by feature
    if "MAPE" in dmet.columns:
        feat_tbl_m = (dmet.groupby(["model","feature"]).MAPE.apply(gmean_pos).reset_index()
                          .pivot(index="model", columns="feature", values="MAPE"))
        save_table(feat_tbl_m, TOUT, "B2_mape_by_feature", index=True, latex_bold_min_cols=list(feat_tbl_m.columns),
                   caption="Geometric-mean MAPE by feature.")

    # Table set C: horizon & span pivots
    # C1: RMSE by horizon (geo-mean over core features & spans)
    if "h" in dmet.columns and "RMSE" in dmet.columns:
        rmse_by_h = (dcore.groupby(["model","h"]).RMSE.apply(gmean_pos).reset_index()
                          .pivot(index="model", columns="h", values="RMSE").sort_index())
        save_table(rmse_by_h, TOUT, "C1_rmse_by_horizon", index=True, latex_bold_min_cols=list(rmse_by_h.columns),
                   caption="RMSE (geo-mean, core features) by forecast horizon (days).")

    # C2: RMSE by span (geo-mean over core features & horizons)
    if "span" in dmet.columns and "RMSE" in dmet.columns:
        rmse_by_span = (dcore.groupby(["model","span"]).RMSE.apply(gmean_pos).reset_index()
                             .pivot(index="model", columns="span", values="RMSE"))
        # order spans if possible
        rmse_by_span = rmse_by_span[[c for c in args.span_order if c in rmse_by_span.columns] + [c for c in rmse_by_span.columns if c not in args.span_order]]
        save_table(rmse_by_span, TOUT, "C2_rmse_by_span", index=True, latex_bold_min_cols=list(rmse_by_span.columns),
                   caption="RMSE (geo-mean, core features) by look-back span.")

    # C3: RMSE by (span,h) per model (flat table)
    if safe_cols(dmet, ["RMSE","span","h","model"]):
        grid = (dcore.groupby(["model","span","h"]).RMSE.apply(gmean_pos).reset_index())
        save_table(grid.sort_values(["model","span","h"]), TOUT, "C3_rmse_by_span_h_grid", index=False,
                   caption="RMSE (geo-mean, core) by model, span, and horizon.")

    # C4: If window 'w' available: RMSE by window size
    if "w" in dmet.columns and "RMSE" in dmet.columns:
        by_w = (dcore.groupby(["model","w"]).RMSE.apply(gmean_pos).reset_index()
                     .pivot(index="model", columns="w", values="RMSE"))
        save_table(by_w, TOUT, "C4_rmse_by_window", index=True, latex_bold_min_cols=list(by_w.columns),
                   caption="RMSE (geo-mean, core) by window size (revolutions/days).")

    # Table set D: baseline-normalised comparisons
    # D1: Median RMSE ratio vs baseline by horizon & feature
    if not dlog.empty and safe_cols(dlog, ["rmse_log","model","file","feature","h","w","span"]):
        base = dlog[dlog.model==baseline][["file","feature","h","w","span","rmse_log"]].rename(columns={"rmse_log":"rmse_log_b"})
        oth = dlog[dlog.model!=baseline]
        mrg = oth.merge(base, on=["file","feature","h","w","span"])
        mrg["ratio"] = np.power(10.0, mrg.rmse_log - mrg.rmse_log_b)
        d_d1 = (mrg.groupby(["model","feature","h"]).ratio.median().reset_index()
                    .pivot_table(index=["model","feature"], columns="h", values="ratio"))
        save_table(d_d1, TOUT, "D1_ratio_vs_baseline_by_feature_h", index=True,
                   caption="Median RMSE ratio vs baseline by feature and horizon (values <1 mean better than baseline).")
    elif safe_cols(dmet, ["model","feature","h","RMSE"]):
        # fallback using aggregated RMSE (less precise)
        agg = dmet.groupby(["model","feature","h"]).RMSE.apply(gmean_pos).reset_index()
        pvt = agg.pivot_table(index=["feature","h"], columns="model", values="RMSE")
        if baseline in pvt.columns:
            ratios = (pvt.div(pvt[baseline], axis=0)).stack().rename("ratio").reset_index().rename(columns={"level_2":"model"})
            d_d1 = ratios.pivot_table(index=["model","feature"], columns="h", values="ratio")
            save_table(d_d1, TOUT, "D1_ratio_vs_baseline_by_feature_h_fallback", index=True,
                       caption="(Fallback) RMSE ratio vs baseline by feature and horizon.")

    # D2: Wins vs baseline counts (how often model beats baseline across matched cells)
    if not dlog.empty and "ratio" in locals():
        wins = (mrg.assign(win=lambda d:(d["ratio"]<1).astype(int))
                    .groupby("model").win.sum().to_frame(name="wins_vs_baseline"))
        total = (mrg.groupby("model").size().to_frame(name="total_cells"))
        d_d2 = wins.join(total)
        d_d2["win_rate_%"] = 100.0 * d_d2["wins_vs_baseline"] / d_d2["total_cells"]
        save_table(d_d2, TOUT, "D2_wins_vs_baseline", index=True,
                   caption="Win counts / rates vs baseline over matched (file,feature,h,span,w) cells.")

    # Table set E: pairwise comparisons across models
    # E1: Pairwise median RMSE ratio matrix
    if safe_cols(dmet, ["model","feature","h","span","RMSE"]):
        key = ["feature","h","span"]
        common = dmet[key].drop_duplicates()
        models = sorted(dmet["model"].unique())
        P = pd.DataFrame(np.nan, index=models, columns=models)
        for m1 in models:
            for m2 in models:
                if m1 == m2: 
                    P.loc[m1,m2] = 1.0
                    continue
                v1 = dmet[dmet.model==m1].merge(common, on=key)["RMSE"].values
                v2 = dmet[dmet.model==m2].merge(common, on=key)["RMSE"].values
                k = min(len(v1), len(v2))
                if k == 0: continue
                P.loc[m1,m2] = np.median(clip_pos(v1[:k]) / clip_pos(v2[:k]))
        save_table(P, TOUT, "E1_pairwise_median_rmse_ratio", index=True,
                   caption="Pairwise median RMSE ratios (row/column). Values <1: row better.")

    # E2: Paired Wilcoxon p-value matrix from per-cell (if dlog available)
    if not dlog.empty and safe_cols(dlog, ["rmse_log","model","file","feature","h","w","span"]):
        pivot_key = ["file","h","feature","span","w"]
        models = sorted(dlog.model.unique())
        P = pd.DataFrame(np.nan, index=models, columns=models)
        # construct per-model RMSE on the common keys
        per = (dlog.assign(RMSE=lambda d: np.power(10.0, d.rmse_log))
                    .groupby(["model"]+pivot_key).RMSE.mean().reset_index())
        common = per[pivot_key].drop_duplicates()
        for i,m1 in enumerate(models):
            v1 = per[per.model==m1].merge(common, on=pivot_key)["RMSE"]
            for j,m2 in enumerate(models):
                if j<=i: 
                    P.loc[m1,m2]=P.loc[m2,m1]= (0.0 if m1==m2 else np.nan)
                    continue
                v2 = per[per.model==m2].merge(common, on=pivot_key)["RMSE"]
                k = min(len(v1), len(v2))
                if k < 10: 
                    P.loc[m1,m2]=P.loc[m2,m1]=np.nan
                    continue
                try:
                    p = stats.wilcoxon(v1[:k], v2[:k], zero_method="wilcox").pvalue
                except Exception:
                    p = np.nan
                P.loc[m1,m2]=P.loc[m2,m1]=p
        save_table(P, TOUT, "E2_pairwise_wilcoxon_pvalues", index=True,
                   caption="Paired Wilcoxon p-values over matched cells.")

    # E3: Best-in-class counts by (span,h,feature)
    if safe_cols(dmet, ["model","feature","h","span","RMSE"]):
        grp = (dmet.groupby(["span","h","feature","model"]).RMSE.apply(gmean_pos).reset_index())
        best = grp.loc[grp.groupby(["span","h","feature"]).RMSE.idxmin()]
        wins = best.groupby("model").size().to_frame(name="best_cell_count").sort_values("best_cell_count", ascending=False)
        save_table(wins, TOUT, "E3_best_in_class_counts", index=True,
                   caption="Counts of (span,h,feature) cells where model is best (lowest RMSE).")

    # Table set F: robustness / quantiles
    # F1: RMSE quantiles (25/50/75) by model & horizon
    if "h" in dmet.columns and "RMSE" in dmet.columns:
        rows=[]
        for (m,h), g in dmet.groupby(["model","h"]):
            rows.append({"model":m,"h":h,"q25":q(g["RMSE"],0.25),"q50":q(g["RMSE"],0.5),"q75":q(g["RMSE"],0.75)})
        rob = pd.DataFrame(rows).pivot(index="model", columns="h", values="q50")
        save_table(rob, TOUT, "F1_rmse_median_by_h", index=True, latex_bold_min_cols=list(rob.columns),
                   caption="Median RMSE by model and horizon.")

    # F2: Error growth ratios (30d / 1d etc.) per feature
    if safe_cols(dmet, ["model","feature","h","RMSE"]):
        targets = [(1,7),(1,30),(1,90),(3,30)]
        rows=[]
        for m,g in dmet.groupby("model"):
            for feat, gf in g.groupby("feature"):
                for a,b in targets:
                    va = g[(g.feature==feat) & (g.h==a)]["RMSE"]
                    vb = g[(g.feature==feat) & (g.h==b)]["RMSE"]
                    if len(va) and len(vb):
                        rows.append({"model":m,"feature":feat,f"{b}d_over_{a}d_ratio": gmean_pos(vb)/gmean_pos(va)})
        if rows:
            growth = pd.DataFrame(rows)
            growth = (growth.groupby(["model","feature"]).mean().reset_index()
                            .pivot(index="model", columns="feature", values=[c for c in growth.columns if "ratio" in c]))
            save_table(growth, TOUT, "F2_error_growth_ratios", index=True,
                       caption="Error growth ratios (higher means larger growth).")

    # Table set G: MAPE analogues (if available)
    if "MAPE" in dmet.columns:
        # G1: MAPE by horizon
        mape_by_h = (dmet.groupby(["model","h"]).MAPE.apply(gmean_pos).reset_index()
                          .pivot(index="model", columns="h", values="MAPE"))
        save_table(mape_by_h, TOUT, "G1_mape_by_horizon", index=True, latex_bold_min_cols=list(mape_by_h.columns),
                   caption="MAPE (geo-mean) by horizon.")
        # G2: MAPE by span
        if "span" in dmet.columns:
            mape_by_span = (dmet.groupby(["model","span"]).MAPE.apply(gmean_pos).reset_index()
                                 .pivot(index="model", columns="span", values="MAPE"))
            save_table(mape_by_span, TOUT, "G2_mape_by_span", index=True, latex_bold_min_cols=list(mape_by_span.columns),
                       caption="MAPE (geo-mean) by span.")

    # Table set H: uncertainty / calibration (if samples contain y_pred_mu/sigma)
    if safe_cols(dsamp, ["y_true","y_pred_mu","y_pred_sigma"]):
        z_grid = np.linspace(0.5, 2.5, 9)
        rows=[]
        for z in z_grid:
            cover = np.mean(np.abs(dsamp.y_true - dsamp.y_pred_mu) <= z*dsamp.y_pred_sigma)
            rows.append({"z":z, "nominal": 2*stats.norm.cdf(z)-1, "empirical": cover})
        calib = pd.DataFrame(rows)
        save_table(calib, TOUT, "H1_uncertainty_coverage_curve", index=False,
                   caption="Nominal vs empirical coverage under Gaussian assumption.")

    # Table set I: resource / cost (if profiles present)
    if not prof.empty and "RMSE" in dmet.columns:
        acc = dmet[dmet.feature.isin(CORE) if set(CORE).issubset(dmet.feature.unique()) else slice(None)].groupby("model").RMSE.apply(gmean_pos).reset_index(name="RMSE_geo")
        p = prof.merge(acc, on="model", how="left")
        keep = [c for c in ["model","params","latency_ms","throughput","flops","RMSE_geo"] if c in p.columns]
        if len(keep) >= 2:
            save_table(p[keep].set_index("model"), TOUT, "I1_resource_vs_accuracy", index=True,
                       caption="Model size/latency vs accuracy (geo-mean RMSE).")

    # Table set J: RTN domain (if available)
    if safe_cols(dsamp, ["dr","dt","dn","h","model"]):
        rows=[]
        for (m,H), g in dsamp.groupby(["model","h"]):
            if g.empty: continue
            at = np.sqrt(np.mean(np.square(g["dt"])))
            norm = np.sqrt(np.mean(g["dr"]**2 + g["dt"]**2 + g["dn"]**2))
            rows.append({"model":m,"h":H,"AT_RMSE_km":at,"Norm3D_RMSE_km":norm})
        rtn = pd.DataFrame(rows).pivot(index="model", columns="h", values="AT_RMSE_km")
        save_table(rtn, TOUT, "J1_rtn_alongtrack_by_h", index=True, latex_bold_min_cols=list(rtn.columns),
                   caption="Along-track RMSE (km) by model and horizon.")
        rtn3 = pd.DataFrame(rows).pivot(index="model", columns="h", values="Norm3D_RMSE_km")
        save_table(rtn3, TOUT, "J2_rtn_norm3d_by_h", index=True, latex_bold_min_cols=list(rtn3.columns),
                   caption="3D RTN norm RMSE (km) by model and horizon.")

    # Table set K: outlier / covariate correlations (if available)
    if safe_cols(dsamp, ["alt_km","ecc","tle_age_d","f107","err","model","h"]):
        # aggregate per sample window to reduce noise
        tri = (dsamp.assign(err2=lambda d:d.err**2)
                    .groupby(["model","h"])
                    .agg(rmse=("err2", lambda s: float(np.sqrt(np.mean(s)))),
                         alt_km=("alt_km","mean"),
                         ecc=("ecc","mean"),
                         tle_age_d=("tle_age_d","mean"),
                         f107=("f107","mean")).reset_index())
        corr_rows=[]
        for m,g in tri.groupby("model"):
            for x in ["alt_km","ecc","tle_age_d","f107"]:
                try:
                    r,pv = stats.pearsonr(g[x].values, g["rmse"].values)
                except Exception:
                    r,pv = (np.nan, np.nan)
                corr_rows.append({"model":m,"covariate":x,"pearson_r":r,"p_value":pv})
        corr = pd.DataFrame(corr_rows).pivot(index="model", columns="covariate", values="pearson_r")
        save_table(corr, TOUT, "K1_covariate_correlation_rmse", index=True,
                   caption="Pearson correlation between RMSE and covariates (per model).")

    # Table set L: training history / overfitting (if available)
    hist_root = Path("results/history")
    frames=[]
    if hist_root.exists():
        for f in hist_root.glob("*.csv"):
            try:
                d=pd.read_csv(f); d["model"]=f.stem
                frames.append(d)
            except: pass
    if frames:
        H=pd.concat(frames, ignore_index=True)
        keep_cols=[c for c in ["model","epoch","val_rmse","train_rmse","val_loss","train_loss"] if c in H.columns]
        # L1: best validation epoch per model
        rows=[]
        if "val_rmse" in H.columns:
            for m,g in H.groupby("model"):
                idx = g["val_rmse"].idxmin()
                r = g.loc[idx].to_dict()
                rows.append({"model":m,"best_epoch":int(r.get("epoch",np.nan)),"best_val_rmse":float(r.get("val_rmse",np.nan))})
            his = pd.DataFrame(rows).set_index("model").sort_values("best_val_rmse")
            save_table(his, TOUT, "L1_best_val_epoch", index=True,
                       caption="Best validation RMSE and epoch per model.")

    # Table set M: CIs over horizons
    if "h" in dmet.columns and "RMSE" in dmet.columns:
        rows=[]
        for (m,h), g in dmet.groupby(["model","h"]):
            mu,lo,hi = bootstrap_ci_gmean(g["RMSE"].values, B=1000, alpha=0.05)
            rows.append({"model":m,"h":h,"gmean_RMSE":mu,"CI_lo":lo,"CI_hi":hi})
        ci_tbl = pd.DataFrame(rows).sort_values(["h","gmean_RMSE"])
        save_table(ci_tbl, TOUT, "M1_rmse_bootstrap_ci_by_h", index=False,
                   caption="Bootstrapped 95\\% CI of geo-mean RMSE by model and horizon.")

    print(f"‚úÖ  Tables written under {TOUT.resolve()}")

if __name__ == "__main__":
    main()
